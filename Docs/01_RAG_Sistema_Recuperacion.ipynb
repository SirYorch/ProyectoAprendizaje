{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22aa4cae",
   "metadata": {},
   "source": [
    "## Arquitectura del Sistema\n",
    "\n",
    "### Base de Datos Vectorial\n",
    "\n",
    "El sistema utiliza **PostgreSQL con pgvector** para almacenar y buscar embeddings:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ      Tabla: rag_embeddings              ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ id              (PK)                    ‚îÇ\n",
    "‚îÇ text            (Texto original)        ‚îÇ\n",
    "‚îÇ group_name      (Categor√≠a)            ‚îÇ\n",
    "‚îÇ intent          (Intenci√≥n espec√≠fica)  ‚îÇ\n",
    "‚îÇ meta            (Metadata JSON)         ‚îÇ\n",
    "‚îÇ embedding       (Vector 768 dims)       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbba30",
   "metadata": {},
   "source": [
    "##  Modelo de Datos - RAGEmbedding\n",
    "\n",
    "La clase `RAGEmbedding` es el modelo ORM que representa la tabla de embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9981f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, Text, String\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "from pgvector.sqlalchemy import Vector\n",
    "\n",
    "class RAGEmbedding(Base):\n",
    "    \"\"\"\n",
    "    Modelo para embeddings del sistema RAG\n",
    "    \n",
    "    Campos:\n",
    "    - text: Texto completo (pregunta + respuesta)\n",
    "    - group_name: Grupo de clasificaci√≥n (ej: 'faq_empresa')\n",
    "    - intent: Intenci√≥n espec√≠fica (ej: 'faq_manual')\n",
    "    - meta: Informaci√≥n adicional en formato JSON\n",
    "    - embedding: Vector de 768 dimensiones generado por Ollama\n",
    "    \"\"\"\n",
    "    __tablename__ = \"rag_embeddings\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    text = Column(Text, nullable=False)\n",
    "    group_name = Column(String(100), nullable=False, index=True)\n",
    "    intent = Column(String(150), nullable=False, index=True)\n",
    "    meta = Column(JSONB, nullable=True)\n",
    "    embedding = Column(Vector(768), nullable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8193a15a",
   "metadata": {},
   "source": [
    "##  Inicializaci√≥n del Sistema\n",
    "\n",
    "Antes de usar el sistema RAG, debemos crear la extensi√≥n pgvector y las tablas necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb141466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "def create_vector_tables():\n",
    "    \"\"\"\n",
    "    Inicializa la base de datos para RAG:\n",
    "    1. Crea la extensi√≥n pgvector en PostgreSQL\n",
    "    2. Crea la tabla rag_embeddings\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        # Habilitar pgvector\n",
    "        conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector;\"))\n",
    "        conn.commit()\n",
    "\n",
    "    # Crear tablas definidas en Base\n",
    "    Base.metadata.create_all(engine)\n",
    "    print(\"‚úì Extensi√≥n vector verificada y tablas creadas\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "# create_vector_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee0db27",
   "metadata": {},
   "source": [
    "##  Inserci√≥n de Embeddings\n",
    "\n",
    "### Inserci√≥n Individual\n",
    "\n",
    "Para agregar un solo embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe3917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embedding(\n",
    "    text: str,\n",
    "    group_name: str,\n",
    "    intent: str,\n",
    "    embedding: list[float],\n",
    "    meta: dict = None\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Inserta un embedding en la base de datos\n",
    "    \n",
    "    Par√°metros:\n",
    "    - text: Texto completo a almacenar\n",
    "    - group_name: Categor√≠a del contenido\n",
    "    - intent: Intenci√≥n espec√≠fica\n",
    "    - embedding: Vector generado (768 dimensiones)\n",
    "    - meta: Metadata adicional (opcional)\n",
    "    \n",
    "    Retorna: ID del registro creado\n",
    "    \"\"\"\n",
    "    session = get_db_session()\n",
    "    \n",
    "    try:\n",
    "        rag = RAGEmbedding(\n",
    "            text=text,\n",
    "            group_name=group_name,\n",
    "            intent=intent,\n",
    "            meta=meta or {},\n",
    "            embedding=embedding\n",
    "        )\n",
    "        session.add(rag)\n",
    "        session.commit()\n",
    "        session.refresh(rag)\n",
    "        return rag.id\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "# Ejemplo de uso\n",
    "# vector = embedder.embed_query(\"¬øCu√°l es el horario de atenci√≥n?\")\n",
    "# add_embedding(\n",
    "#     text=\"Pregunta: ¬øHorarios? Respuesta: 9am-6pm lun-vie\",\n",
    "#     group_name=\"faq_empresa\",\n",
    "#     intent=\"faq_manual\",\n",
    "#     embedding=vector,\n",
    "#     meta={\"tipo\": \"horarios\"}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce1544",
   "metadata": {},
   "source": [
    "### Inserci√≥n por Lotes\n",
    "\n",
    "Para optimizar la carga masiva de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_batch_embeddings(items: list[dict]) -> int:\n",
    "    \"\"\"\n",
    "    Inserta m√∫ltiples embeddings en una sola transacci√≥n\n",
    "    \n",
    "    Cada item debe tener:\n",
    "    {\n",
    "        \"text\": str,\n",
    "        \"group_name\": str,\n",
    "        \"intent\": str,\n",
    "        \"embedding\": list[float],\n",
    "        \"meta\": dict (opcional)\n",
    "    }\n",
    "    \n",
    "    Retorna: Cantidad de registros insertados\n",
    "    \"\"\"\n",
    "    session = get_db_session()\n",
    "    \n",
    "    try:\n",
    "        objs = [\n",
    "            RAGEmbedding(\n",
    "                text=it[\"text\"],\n",
    "                group_name=it[\"group_name\"],\n",
    "                intent=it[\"intent\"],\n",
    "                embedding=it[\"embedding\"],\n",
    "                meta=it.get(\"meta\", {})\n",
    "            )\n",
    "            for it in items\n",
    "        ]\n",
    "        \n",
    "        session.add_all(objs)\n",
    "        session.commit()\n",
    "        return len(objs)\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00bbcfe",
   "metadata": {},
   "source": [
    "##  B√∫squeda Sem√°ntica\n",
    "\n",
    "La funci√≥n m√°s importante del sistema: buscar informaci√≥n relevante bas√°ndose en similitud vectorial.\n",
    "\n",
    "### Distancia Coseno\n",
    "\n",
    "Utilizamos **distancia coseno** para medir similitud:\n",
    "- **0** = Vectores id√©nticos (m√°xima similitud)\n",
    "- **1** = Vectores opuestos (m√≠nima similitud)\n",
    "\n",
    "### Umbral de Relevancia\n",
    "\n",
    "El par√°metro `threshold` (0.4 por defecto) filtra resultados irrelevantes. Solo devuelve matches con distancia < threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(\n",
    "    query_embedding: list[float],\n",
    "    top_k: int = 3,\n",
    "    group_filter: str = None,\n",
    "    intent_filter: str = None,\n",
    "    threshold: float = 0.4\n",
    ") -> list[tuple]:\n",
    "    \"\"\"\n",
    "    Busca los embeddings m√°s similares a la consulta\n",
    "    \n",
    "    Par√°metros:\n",
    "    - query_embedding: Vector de la pregunta del usuario\n",
    "    - top_k: Cantidad m√°xima de resultados\n",
    "    - group_filter: Filtrar por grupo espec√≠fico\n",
    "    - intent_filter: Filtrar por intenci√≥n espec√≠fica\n",
    "    - threshold: Umbral de distancia (0-1, menor = m√°s estricto)\n",
    "    \n",
    "    Retorna: Lista de tuplas [(RAGEmbedding, distancia), ...]\n",
    "    \"\"\"\n",
    "    session = get_db_session()\n",
    "    \n",
    "    try:\n",
    "        # Calcular distancia coseno\n",
    "        distance_expr = RAGEmbedding.embedding.cosine_distance(query_embedding)\n",
    "        \n",
    "        # Construir query\n",
    "        query = session.query(\n",
    "            RAGEmbedding,\n",
    "            distance_expr.label(\"distance\")\n",
    "        )\n",
    "        \n",
    "        # Aplicar filtros\n",
    "        if group_filter:\n",
    "            query = query.filter(RAGEmbedding.group_name == group_filter)\n",
    "        \n",
    "        if intent_filter:\n",
    "            query = query.filter(RAGEmbedding.intent == intent_filter)\n",
    "        \n",
    "        # CLAVE: Filtrar por umbral de relevancia\n",
    "        query = query.filter(distance_expr < threshold)\n",
    "        \n",
    "        # Ordenar por similitud y limitar resultados\n",
    "        results = query.order_by(distance_expr).limit(top_k).all()\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en b√∫squeda: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be74cba0",
   "metadata": {},
   "source": [
    "##  Ingesta de FAQs\n",
    "\n",
    "El archivo `ingestor.py` procesa las FAQs desde un archivo JSON y las convierte en embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a89bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Configurar el modelo de embeddings\n",
    "embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "def ingest_faqs(path=\"files/faqs.json\"):\n",
    "    \"\"\"\n",
    "    Proceso de ingesta de FAQs:\n",
    "    1. Leer archivo JSON con preguntas y respuestas\n",
    "    2. Generar embeddings para cada pregunta\n",
    "    3. Almacenar en la base de datos vectorial\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cargar FAQs\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lista_faqs = json.load(f)\n",
    "    \n",
    "    print(f\"Procesando {len(lista_faqs)} FAQs...\")\n",
    "    \n",
    "    items_to_insert = []\n",
    "    \n",
    "    # Extraer solo las preguntas para vectorizaci√≥n\n",
    "    preguntas = [item[\"pregunta\"] for item in lista_faqs]\n",
    "    \n",
    "    # Generar embeddings en lote (m√°s eficiente)\n",
    "    print(\"Generando embeddings...\")\n",
    "    vectores = embedder.embed_documents(preguntas)\n",
    "    \n",
    "    # Preparar datos para inserci√≥n\n",
    "    for i, item in enumerate(lista_faqs):\n",
    "        faq_data = {\n",
    "            # Texto completo: pregunta + respuesta\n",
    "            \"text\": f\"Pregunta: {item['pregunta']}\\nRespuesta: {item['respuesta']}\",\n",
    "            \"group_name\": \"faq_empresa\",\n",
    "            \"intent\": \"faq_manual\",\n",
    "            \"embedding\": vectores[i],\n",
    "            \"meta\": {\"tipo\": \"faq_estatica\"}\n",
    "        }\n",
    "        items_to_insert.append(faq_data)\n",
    "    \n",
    "    # Insertar en la BD\n",
    "    add_batch_embeddings(items_to_insert)\n",
    "    print(\" FAQs cargadas exitosamente!\")\n",
    "\n",
    "# Ejecutar ingesta\n",
    "# ingest_faqs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb860c",
   "metadata": {},
   "source": [
    "##  Pruebas del Sistema\n",
    "\n",
    "Ejemplo de c√≥mo probar el sistema RAG con preguntas de usuarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5713213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preguntas de prueba\n",
    "preguntas_test = [\n",
    "    \"¬øA qu√© hora abren la tienda?\",\n",
    "    \"¬øTienen env√≠os a Guayaquil?\",\n",
    "    \"Se me da√±√≥ el producto, ¬øqu√© hago?\",\n",
    "    \"¬øCu√°l es la mejor compu que tienen?\",  # No est√° en FAQs\n",
    "]\n",
    "\n",
    "for pregunta in preguntas_test:\n",
    "    print(f\"\\n Usuario: '{pregunta}'\")\n",
    "    \n",
    "    # Generar embedding de la pregunta\n",
    "    query_vector = embedder.embed_query(pregunta)\n",
    "    \n",
    "    # Buscar en la base de conocimiento\n",
    "    resultados = similarity_search(\n",
    "        query_embedding=query_vector,\n",
    "        top_k=1,\n",
    "        group_filter=\"faq_empresa\",\n",
    "        threshold=0.5  # Ajustar seg√∫n necesidad\n",
    "    )\n",
    "    \n",
    "    if not resultados:\n",
    "        print(\" No encontr√© informaci√≥n relevante en la base de conocimiento.\")\n",
    "    else:\n",
    "        mejor_match, distancia = resultados[0]\n",
    "        print(f\" Distancia: {distancia:.4f}\")\n",
    "        print(f\"üìÑ Respuesta: {mejor_match.text}\")\n",
    "    \n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7101a84",
   "metadata": {},
   "source": [
    "##  Flujo de Trabajo Completo\n",
    "\n",
    "```\n",
    "1. INICIALIZACI√ìN\n",
    "   ‚îî‚îÄ> create_vector_tables()\n",
    "       ‚îî‚îÄ> Crea extensi√≥n pgvector\n",
    "       ‚îî‚îÄ> Crea tabla rag_embeddings\n",
    "\n",
    "2. INGESTA DE DATOS\n",
    "   ‚îî‚îÄ> ingest_faqs()\n",
    "       ‚îî‚îÄ> Lee faqs.json\n",
    "       ‚îî‚îÄ> Genera embeddings con Ollama\n",
    "       ‚îî‚îÄ> add_batch_embeddings()\n",
    "\n",
    "3. CONSULTA DE USUARIO\n",
    "   ‚îî‚îÄ> Usuario hace pregunta\n",
    "       ‚îî‚îÄ> embedder.embed_query(pregunta)\n",
    "       ‚îî‚îÄ> similarity_search()\n",
    "           ‚îî‚îÄ> Calcula distancia coseno\n",
    "           ‚îî‚îÄ> Filtra por umbral\n",
    "           ‚îî‚îÄ> Retorna mejores matches\n",
    "       ‚îî‚îÄ> Retornar respuesta al usuario\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864daf15",
   "metadata": {},
   "source": [
    "##  Configuraci√≥n y Optimizaci√≥n\n",
    "\n",
    "### Par√°metros Clave\n",
    "\n",
    "| Par√°metro | Valor | Descripci√≥n |\n",
    "|-----------|-------|-------------|\n",
    "| `model` | nomic-embed-text | Modelo de Ollama para embeddings |\n",
    "| `vector_dim` | 768 | Dimensiones del vector |\n",
    "| `threshold` | 0.4-0.5 | Umbral de relevancia |\n",
    "| `top_k` | 1-3 | Cantidad de resultados |\n",
    "\n",
    "### Ajuste del Threshold\n",
    "\n",
    "- **0.3 o menos**: Muy estricto, solo matches casi exactos\n",
    "- **0.4-0.5**: Balance entre precisi√≥n y cobertura (recomendado)\n",
    "- **0.6 o m√°s**: Permisivo, puede devolver resultados irrelevantes\n",
    "\n",
    "### Mejores Pr√°cticas\n",
    "\n",
    "1. **Preparar bien el texto**: Incluir pregunta + respuesta en el campo `text`\n",
    "2. **Usar filtros**: Aprovechar `group_name` e `intent` para b√∫squedas focalizadas\n",
    "3. **Ajustar threshold**: Probar con datos reales y ajustar seg√∫n resultados\n",
    "4. **Indexar correctamente**: Usar √≠ndices HNSW para b√∫squedas r√°pidas\n",
    "5. **Metadata √∫til**: Guardar informaci√≥n adicional en `meta` para contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfc0d06",
   "metadata": {},
   "source": [
    "##  Ventajas del Sistema RAG\n",
    "\n",
    " **Respuestas Precisas**: Basadas en informaci√≥n real de la empresa\n",
    "\n",
    " **Escalable**: F√°cil agregar m√°s FAQs sin reentrenar modelos\n",
    "\n",
    " **R√°pido**: B√∫squeda vectorial optimizada con pgvector\n",
    "\n",
    " **Controlable**: Umbral de relevancia evita respuestas incorrectas\n",
    "\n",
    " **Actualizable**: Cambios en FAQs se reflejan inmediatamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41f8d6",
   "metadata": {},
   "source": [
    "##  Referencias\n",
    "\n",
    "- **pgvector**: https://github.com/pgvector/pgvector\n",
    "- **Ollama Embeddings**: https://ollama.ai/\n",
    "- **LangChain**: https://python.langchain.com/\n",
    "- **RAG Pattern**: https://arxiv.org/abs/2005.11401\n",
    "\n",
    "---\n",
    "\n",
    "*Documentaci√≥n generada para ProyectoAprendizaje - Diciembre 2024*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
