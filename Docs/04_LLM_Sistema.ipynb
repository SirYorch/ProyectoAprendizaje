{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81cdc906",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Arquitectura del Sistema\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ               PIPELINE DE PROCESAMIENTO                  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                          ‚îÇ\n",
    "‚îÇ  Mensaje Usuario                                         ‚îÇ\n",
    "‚îÇ       ‚Üì                                                  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                ‚îÇ\n",
    "‚îÇ  ‚îÇ  1. REGEX           ‚îÇ ‚Üê Saludos, despedidas         ‚îÇ\n",
    "‚îÇ  ‚îÇ  (agent.py)         ‚îÇ   agradecimientos             ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                ‚îÇ\n",
    "‚îÇ       ‚Üì (si no match)                                    ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                ‚îÇ\n",
    "‚îÇ  ‚îÇ  2. RAG             ‚îÇ ‚Üê FAQs de la empresa          ‚îÇ\n",
    "‚îÇ  ‚îÇ  (embeddings.py)    ‚îÇ                                ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                ‚îÇ\n",
    "‚îÇ       ‚Üì (si no match)                                    ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                ‚îÇ\n",
    "‚îÇ  ‚îÇ  3. FUNCTION        ‚îÇ ‚Üê Predicciones, reportes      ‚îÇ\n",
    "‚îÇ  ‚îÇ  MATCHER            ‚îÇ                                ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                ‚îÇ\n",
    "‚îÇ       ‚Üì                                                  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                ‚îÇ\n",
    "‚îÇ  ‚îÇ  4. LLM GEMINI      ‚îÇ ‚Üê Naturalizaci√≥n de           ‚îÇ\n",
    "‚îÇ  ‚îÇ  (llm.py)           ‚îÇ   respuestas                   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                ‚îÇ\n",
    "‚îÇ       ‚Üì                                                  ‚îÇ\n",
    "‚îÇ  Respuesta Final                                         ‚îÇ\n",
    "‚îÇ                                                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e80f6c",
   "metadata": {},
   "source": [
    "## ü§ñ Parte 1: Integraci√≥n con Google Gemini\n",
    "\n",
    "El archivo `llm.py` contiene la integraci√≥n con la API de Google Gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2838f3",
   "metadata": {},
   "source": [
    "### Configuraci√≥n Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ccaf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import os\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Inicializar cliente de Gemini\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7cd8dc",
   "metadata": {},
   "source": [
    "### Funci√≥n Principal: naturalize_response()\n",
    "\n",
    "Esta funci√≥n toma datos t√©cnicos y los convierte en respuestas naturales y amigables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0607678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naturalize_response(base, presentation=False):\n",
    "    \"\"\"\n",
    "    Naturaliza respuestas usando Google Gemini.\n",
    "    \n",
    "    Args:\n",
    "        base: Datos t√©cnicos (dict, list, string) a naturalizar\n",
    "        presentation: Si True, genera una presentaci√≥n del asistente\n",
    "    \n",
    "    Returns:\n",
    "        str: Respuesta naturalizada en lenguaje humano\n",
    "    \n",
    "    Ejemplos:\n",
    "        Base t√©cnica:\n",
    "        {\n",
    "            'producto': 'Laptop HP',\n",
    "            'stock_actual': 45,\n",
    "            'dias_hasta_agotarse': 12\n",
    "        }\n",
    "        \n",
    "        Respuesta naturalizada:\n",
    "        \"Actualmente tenemos 45 unidades de Laptop HP en stock.\n",
    "         Seg√∫n las ventas recientes, se estima que el producto\n",
    "         se agotar√° en aproximadamente 12 d√≠as.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    if presentation:\n",
    "        # Mensaje de presentaci√≥n del asistente\n",
    "        message = (\n",
    "            \"Eres un asistente de ventas para una empresa de electr√≥nicos \"\n",
    "            \"llamada Arc -- tienda de electr√≥nicos avanzada, que puede buscar \"\n",
    "            \"en documentos de preguntas frecuentes, o predecir stock, o generar \"\n",
    "            \"reportes, presentate\"\n",
    "        )\n",
    "    else:\n",
    "        # Naturalizaci√≥n de datos\n",
    "        message = (\n",
    "            \"eres un asistente de ventas para una empresa de electr√≥nicos \"\n",
    "            \"de una forma amigable y c√≥moda, no uses decoradores de texto, \"\n",
    "            \"es decir escribe principalmente lo necesario de forma amigable, \"\n",
    "            \"y si el contenido no existe, da un mensaje de error. \"\n",
    "            f\"Debes presentarle al usuario la siguiente informaci√≥n: {str(base)} \"\n",
    "            \"aqui terminan los datos, si estos se encuentran vac√≠os, presenta \"\n",
    "            \"un mensaje de error, en lugar de datos incorrectos\"\n",
    "        )\n",
    "    \n",
    "    # Llamada a Gemini\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=message\n",
    "    )\n",
    "    \n",
    "    print(response.text)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e681e92",
   "metadata": {},
   "source": [
    "### Ejemplos de Uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e78448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 1: Presentaci√≥n del asistente\n",
    "presentacion = naturalize_response(None, presentation=True)\n",
    "print(\"=\"*70)\n",
    "print(presentacion)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ejemplo 2: Naturalizar predicci√≥n de stock\n",
    "datos_stock = {\n",
    "    'producto': 'Laptop HP',\n",
    "    'stock_actual': 45,\n",
    "    'prediccion': [\n",
    "        {'fecha': '2024-12-08', 'stock': 42},\n",
    "        {'fecha': '2024-12-09', 'stock': 39},\n",
    "        {'fecha': '2024-12-10', 'stock': 36}\n",
    "    ]\n",
    "}\n",
    "\n",
    "respuesta = naturalize_response(datos_stock)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(respuesta)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ejemplo 3: Naturalizar top productos\n",
    "datos_top = {\n",
    "    'tipo': 'top_vendidos',\n",
    "    'productos': [\n",
    "        {'nombre': 'Mouse Logitech', 'ventas': 156},\n",
    "        {'nombre': 'Teclado Mec√°nico', 'ventas': 142},\n",
    "        {'nombre': 'Monitor Samsung', 'ventas': 98}\n",
    "    ]\n",
    "}\n",
    "\n",
    "respuesta = naturalize_response(datos_top)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(respuesta)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ejemplo 4: Datos vac√≠os (manejo de errores)\n",
    "respuesta = naturalize_response({})\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(respuesta)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a690d608",
   "metadata": {},
   "source": [
    "## üéØ Parte 2: Agente Conversacional\n",
    "\n",
    "El archivo `agent.py` combina regex con el procesamiento de mensajes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c8322",
   "metadata": {},
   "source": [
    "### Sistema de Detecci√≥n por Regex\n",
    "\n",
    "Ya cubierto en el notebook de Expresiones Regulares, pero aqu√≠ vemos c√≥mo se integra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf22eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def check_regex_response(user_text: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Primera l√≠nea de defensa: respuestas r√°pidas con regex.\n",
    "    \n",
    "    Retorna:\n",
    "    - str: Respuesta inmediata\n",
    "    - None: Continuar con RAG/Function Matcher/LLM\n",
    "    \"\"\"\n",
    "    text = user_text.lower()\n",
    "\n",
    "    # SALUDOS\n",
    "    patron_saludos = r\"\\b(hola|oli|buenos d[√≠i]as|buenas tardes|buenas noches|que tal|hello)\\b\"\n",
    "    if re.search(patron_saludos, text):\n",
    "        respuestas = [\n",
    "            \"¬°Hola! Bienvenido a Nombre. ¬øEn qu√© puedo ayudarte hoy?\",\n",
    "            \"¬°Buenas! Soy tu asistente virtual. ¬øBuscas stock o informaci√≥n?\",\n",
    "            \"¬°Hola! Estoy listo para ayudarte con el inventario.\"\n",
    "        ]\n",
    "        return random.choice(respuestas)\n",
    "\n",
    "    # DESPEDIDAS\n",
    "    patron_despedidas = r\"\\b(chao|chau|adi[√≥o]s|hasta luego|nos vemos|bye|cu[√≠i]date)\\b\"\n",
    "    if re.search(patron_despedidas, text):\n",
    "        respuestas = [\n",
    "            \"¬°Hasta luego! Gracias por visitar Nombre.\",\n",
    "            \"¬°Chao! Vuelve pronto.\",\n",
    "            \"Nos vemos. Espero haberte ayudado.\"\n",
    "        ]\n",
    "        return random.choice(respuestas)\n",
    "\n",
    "    # AGRADECIMIENTOS\n",
    "    patron_agradecimientos = r\"\\b(gracias|te agradezco|muy amable|thx)\\b\"\n",
    "    if re.search(patron_agradecimientos, text):\n",
    "        respuestas = [\n",
    "            \"¬°De nada! Es un placer ayudarte.\",\n",
    "            \"¬°Para eso estamos!\",\n",
    "            \"Con gusto. ¬øNecesitas algo m√°s?\"\n",
    "        ]\n",
    "        return random.choice(respuestas)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5d8c33",
   "metadata": {},
   "source": [
    "## üîÑ Pipeline Completo de Procesamiento\n",
    "\n",
    "As√≠ es como se integra todo en el flujo real del chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd2ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_message(mensaje: str) -> str:\n",
    "    \"\"\"\n",
    "    Pipeline completo de procesamiento de mensajes.\n",
    "    \n",
    "    Orden de prioridad:\n",
    "    1. Regex (saludos, despedidas, agradecimientos)\n",
    "    2. RAG (b√∫squeda en FAQs)\n",
    "    3. Function Matcher (identificar funciones)\n",
    "    4. LLM Gemini (naturalizar respuesta o conversaci√≥n general)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========================================\n",
    "    # PASO 1: Verificar patrones regex simples\n",
    "    # ========================================\n",
    "    respuesta_regex = check_regex_response(mensaje)\n",
    "    if respuesta_regex:\n",
    "        print(\"[REGEX] Respuesta r√°pida\")\n",
    "        return respuesta_regex\n",
    "    \n",
    "    # ========================================\n",
    "    # PASO 2: Buscar en RAG (FAQs)\n",
    "    # ========================================\n",
    "    from rag.embeddings import similarity_search\n",
    "    from langchain_ollama import OllamaEmbeddings\n",
    "    \n",
    "    embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    query_vector = embedder.embed_query(mensaje)\n",
    "    \n",
    "    resultados_rag = similarity_search(\n",
    "        query_embedding=query_vector,\n",
    "        top_k=1,\n",
    "        group_filter=\"faq_empresa\",\n",
    "        threshold=0.5\n",
    "    )\n",
    "    \n",
    "    if resultados_rag:\n",
    "        mejor_match, distancia = resultados_rag[0]\n",
    "        print(f\"[RAG] Match encontrado (distancia: {distancia:.4f})\")\n",
    "        return mejor_match.text\n",
    "    \n",
    "    # ========================================\n",
    "    # PASO 3: Function Matcher\n",
    "    # ========================================\n",
    "    from ai.matcher import FunctionCaller\n",
    "    \n",
    "    caller = FunctionCaller()\n",
    "    resultado = caller.identificar_funcion(mensaje)\n",
    "    \n",
    "    if resultado['funcion'] and resultado['confianza'] >= 0.6:\n",
    "        print(f\"[FUNCTION] {resultado['funcion']} identificada\")\n",
    "        \n",
    "        # Ejecutar la funci√≥n (pseudoc√≥digo)\n",
    "        # datos = ejecutar_funcion(resultado['funcion'], resultado['parametros'])\n",
    "        \n",
    "        # Simular datos de respuesta\n",
    "        datos = {\n",
    "            'funcion': resultado['funcion'],\n",
    "            'parametros': resultado['parametros'],\n",
    "            'resultado': 'Datos de la funci√≥n...'\n",
    "        }\n",
    "        \n",
    "        # Naturalizar con LLM\n",
    "        print(\"[LLM] Naturalizando respuesta...\")\n",
    "        return naturalize_response(datos)\n",
    "    \n",
    "    # ========================================\n",
    "    # PASO 4: Conversaci√≥n general con LLM\n",
    "    # ========================================\n",
    "    print(\"[LLM] Generando respuesta conversacional...\")\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=(\n",
    "            \"Eres un asistente de ventas para Arc - tienda de electr√≥nicos. \"\n",
    "            \"Responde de forma amigable y profesional. \"\n",
    "            f\"Usuario: {mensaje}\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3156cec8",
   "metadata": {},
   "source": [
    "## üß™ Pruebas del Sistema Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa130e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casos de prueba diversos\n",
    "test_cases = [\n",
    "    # Regex - Saludos\n",
    "    \"Hola, buenos d√≠as\",\n",
    "    \n",
    "    # RAG - FAQs\n",
    "    \"¬øCu√°l es el horario de atenci√≥n?\",\n",
    "    \"¬øAceptan devoluciones?\",\n",
    "    \n",
    "    # Function Matcher\n",
    "    \"¬øCu√°nto stock hay de Laptop HP?\",\n",
    "    \"Productos m√°s vendidos\",\n",
    "    \"Stock para el 25 de diciembre\",\n",
    "    \n",
    "    # LLM General\n",
    "    \"¬øPor qu√© deber√≠a comprar en su tienda?\",\n",
    "    \"Cu√©ntame sobre laptops\",\n",
    "    \n",
    "    # Regex - Despedidas\n",
    "    \"Gracias, adi√≥s\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PRUEBA COMPLETA DEL SISTEMA LLM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, mensaje in enumerate(test_cases, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{i}/{len(test_cases)}] üë§ Usuario: '{mensaje}'\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    respuesta = process_user_message(mensaje)\n",
    "    \n",
    "    print(f\"\\nü§ñ Asistente:\\n{respuesta}\")\n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a065020",
   "metadata": {},
   "source": [
    "## üìä M√©tricas de Rendimiento\n",
    "\n",
    "### Comparaci√≥n de Tiempos de Respuesta\n",
    "\n",
    "| M√©todo | Tiempo Promedio | Costo | Precisi√≥n |\n",
    "|--------|----------------|-------|------------|\n",
    "| **Regex** | < 1ms | $0.00 | 100% (patrones exactos) |\n",
    "| **RAG** | 10-50ms | $0.00 | 85-95% |\n",
    "| **Function Matcher** | 20-80ms | $0.00 | 85-92% |\n",
    "| **LLM Gemini** | 500-2000ms | $0.001-0.01 | 90-98% |\n",
    "\n",
    "### Distribuci√≥n de Uso (Ejemplo)\n",
    "\n",
    "```\n",
    "Regex:           15% ‚Üê‚îÄ Respuestas instant√°neas\n",
    "RAG:             35% ‚Üê‚îÄ FAQs y conocimiento base\n",
    "Function Match:  40% ‚Üê‚îÄ Funcionalidad principal\n",
    "LLM:            10% ‚Üê‚îÄ Conversaci√≥n general\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3368bd76",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuraci√≥n y Variables de Entorno\n",
    "\n",
    "### Archivo .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60305621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env\n",
    "\"\"\"\n",
    "# Google Gemini API\n",
    "GOOGLE_GEMINI_API_KEY=tu_api_key_aqui\n",
    "\n",
    "# PostgreSQL\n",
    "DATABASE_URL=postgresql://usuario1:password1@localhost:5432/aprendizaje\n",
    "\n",
    "# Ollama (para embeddings)\n",
    "OLLAMA_BASE_URL=http://localhost:11434\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5a92a9",
   "metadata": {},
   "source": [
    "### Instalaci√≥n de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a977a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txt\n",
    "\"\"\"\n",
    "google-generativeai\n",
    "python-dotenv\n",
    "langchain-ollama\n",
    "sentence-transformers\n",
    "sqlalchemy\n",
    "pgvector\n",
    "psycopg2-binary\n",
    "\"\"\"\n",
    "\n",
    "# Instalaci√≥n\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b1626",
   "metadata": {},
   "source": [
    "## üé® Personalizaci√≥n del Asistente\n",
    "\n",
    "### Ajustar Personalidad y Tono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5575668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sistema de prompts modulares\n",
    "\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"base\": (\n",
    "        \"Eres un asistente de ventas para Arc - tienda de electr√≥nicos avanzada. \"\n",
    "    ),\n",
    "    \n",
    "    \"tono_amigable\": (\n",
    "        \"Usa un tono amigable, cercano y profesional. \"\n",
    "        \"S√© conciso pero completo en tus respuestas. \"\n",
    "    ),\n",
    "    \n",
    "    \"formato\": (\n",
    "        \"No uses decoradores de texto como **negrita** o *cursiva*. \"\n",
    "        \"Escribe en texto plano pero bien estructurado. \"\n",
    "    ),\n",
    "    \n",
    "    \"manejo_errores\": (\n",
    "        \"Si no tienes informaci√≥n o los datos est√°n vac√≠os, \"\n",
    "        \"comunica el error de forma amable y ofrece alternativas. \"\n",
    "    )\n",
    "}\n",
    "\n",
    "def build_prompt(datos, tipo=\"respuesta\"):\n",
    "    \"\"\"Construye prompt personalizado seg√∫n el tipo de interacci√≥n\"\"\"\n",
    "    \n",
    "    base = SYSTEM_PROMPTS[\"base\"]\n",
    "    tono = SYSTEM_PROMPTS[\"tono_amigable\"]\n",
    "    formato = SYSTEM_PROMPTS[\"formato\"]\n",
    "    errores = SYSTEM_PROMPTS[\"manejo_errores\"]\n",
    "    \n",
    "    if tipo == \"presentacion\":\n",
    "        return f\"{base} {tono} Pres√©ntate brevemente al usuario.\"\n",
    "    \n",
    "    elif tipo == \"respuesta\":\n",
    "        return (\n",
    "            f\"{base} {tono} {formato} {errores} \"\n",
    "            f\"Presenta la siguiente informaci√≥n al usuario: {datos}\"\n",
    "        )\n",
    "    \n",
    "    elif tipo == \"conversacion\":\n",
    "        return f\"{base} {tono} Responde a la consulta del usuario.\"\n",
    "\n",
    "# Uso\n",
    "# prompt = build_prompt(datos_stock, tipo=\"respuesta\")\n",
    "# response = client.models.generate_content(model=\"gemini-2.5-flash\", contents=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716f4d9",
   "metadata": {},
   "source": [
    "## üîí Mejores Pr√°cticas de Seguridad\n",
    "\n",
    "### 1. Manejo Seguro de API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80329e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# NUNCA hardcodear API keys\n",
    "# ‚ùå MAL\n",
    "# api_key = \"AIzaSyC123456789...\"\n",
    "\n",
    "# ‚úÖ BIEN\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_GEMINI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key no configurada en .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd573e3",
   "metadata": {},
   "source": [
    "### 2. Validaci√≥n de Entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc1315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_user_input(mensaje: str) -> bool:\n",
    "    \"\"\"\n",
    "    Valida entrada del usuario antes de procesarla.\n",
    "    \"\"\"\n",
    "    # Longitud m√°xima\n",
    "    if len(mensaje) > 500:\n",
    "        raise ValueError(\"Mensaje demasiado largo (m√°x. 500 caracteres)\")\n",
    "    \n",
    "    # No vac√≠o\n",
    "    if not mensaje.strip():\n",
    "        raise ValueError(\"Mensaje vac√≠o\")\n",
    "    \n",
    "    # Detectar posibles inyecciones (b√°sico)\n",
    "    palabras_prohibidas = [\"<script>\", \"DROP TABLE\", \"DELETE FROM\"]\n",
    "    mensaje_lower = mensaje.lower()\n",
    "    \n",
    "    for palabra in palabras_prohibidas:\n",
    "        if palabra.lower() in mensaje_lower:\n",
    "            raise ValueError(\"Entrada inv√°lida detectada\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568ef05",
   "metadata": {},
   "source": [
    "### 3. Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"\n",
    "    Limita la cantidad de peticiones por usuario.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_requests=10, time_window=60):\n",
    "        self.max_requests = max_requests  # M√°ximo de peticiones\n",
    "        self.time_window = time_window    # Ventana de tiempo (segundos)\n",
    "        self.requests = defaultdict(list)\n",
    "    \n",
    "    def is_allowed(self, user_id: str) -> bool:\n",
    "        \"\"\"Verifica si el usuario puede hacer otra petici√≥n\"\"\"\n",
    "        now = datetime.now()\n",
    "        \n",
    "        # Limpiar peticiones antiguas\n",
    "        cutoff = now - timedelta(seconds=self.time_window)\n",
    "        self.requests[user_id] = [\n",
    "            req_time for req_time in self.requests[user_id]\n",
    "            if req_time > cutoff\n",
    "        ]\n",
    "        \n",
    "        # Verificar l√≠mite\n",
    "        if len(self.requests[user_id]) >= self.max_requests:\n",
    "            return False\n",
    "        \n",
    "        # Registrar nueva petici√≥n\n",
    "        self.requests[user_id].append(now)\n",
    "        return True\n",
    "\n",
    "# Uso\n",
    "limiter = RateLimiter(max_requests=10, time_window=60)\n",
    "\n",
    "# if not limiter.is_allowed(user_id):\n",
    "#     return \"Has excedido el l√≠mite de peticiones. Intenta en un minuto.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0b728",
   "metadata": {},
   "source": [
    "## üìà Monitoreo y Logging\n",
    "\n",
    "### Sistema de Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35446120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('chatbot.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_interaction(user_id: str, mensaje: str, respuesta: str, metodo: str):\n",
    "    \"\"\"\n",
    "    Registra cada interacci√≥n del chatbot.\n",
    "    \"\"\"\n",
    "    logger.info(f\"\"\"\\n\n",
    "    {'='*50}\n",
    "    INTERACCI√ìN\n",
    "    {'='*50}\n",
    "    Timestamp: {datetime.now().isoformat()}\n",
    "    User ID: {user_id}\n",
    "    M√©todo: {metodo}\n",
    "    \n",
    "    Mensaje: {mensaje}\n",
    "    Respuesta: {respuesta[:100]}...\n",
    "    {'='*50}\n",
    "    \"\"\")\n",
    "\n",
    "# Uso en el pipeline\n",
    "# log_interaction(user_id, mensaje, respuesta, \"REGEX\")\n",
    "# log_interaction(user_id, mensaje, respuesta, \"RAG\")\n",
    "# log_interaction(user_id, mensaje, respuesta, \"FUNCTION_MATCHER\")\n",
    "# log_interaction(user_id, mensaje, respuesta, \"LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e120517",
   "metadata": {},
   "source": [
    "## üöÄ Optimizaciones de Rendimiento\n",
    "\n",
    "### 1. Cach√© de Respuestas Frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import hashlib\n",
    "\n",
    "class ResponseCache:\n",
    "    \"\"\"\n",
    "    Cach√© simple para respuestas frecuentes.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size=100):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def _hash_message(self, mensaje: str) -> str:\n",
    "        \"\"\"Genera hash del mensaje para usar como key\"\"\"\n",
    "        return hashlib.md5(mensaje.lower().strip().encode()).hexdigest()\n",
    "    \n",
    "    def get(self, mensaje: str):\n",
    "        \"\"\"Obtiene respuesta del cach√©\"\"\"\n",
    "        key = self._hash_message(mensaje)\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, mensaje: str, respuesta: str):\n",
    "        \"\"\"Guarda respuesta en cach√©\"\"\"\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            # Eliminar el m√°s antiguo (FIFO simple)\n",
    "            self.cache.pop(next(iter(self.cache)))\n",
    "        \n",
    "        key = self._hash_message(mensaje)\n",
    "        self.cache[key] = respuesta\n",
    "\n",
    "# Uso\n",
    "cache = ResponseCache(max_size=100)\n",
    "\n",
    "def process_with_cache(mensaje: str) -> str:\n",
    "    # Verificar cach√©\n",
    "    cached = cache.get(mensaje)\n",
    "    if cached:\n",
    "        logger.info(\"[CACHE] Respuesta encontrada en cach√©\")\n",
    "        return cached\n",
    "    \n",
    "    # Procesar normalmente\n",
    "    respuesta = process_user_message(mensaje)\n",
    "    \n",
    "    # Guardar en cach√©\n",
    "    cache.set(mensaje, respuesta)\n",
    "    \n",
    "    return respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a444a0f",
   "metadata": {},
   "source": [
    "### 2. Procesamiento As√≠ncrono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc15b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def process_user_message_async(mensaje: str) -> str:\n",
    "    \"\"\"\n",
    "    Versi√≥n as√≠ncrona del pipeline.\n",
    "    Permite procesar m√∫ltiples mensajes concurrentemente.\n",
    "    \"\"\"\n",
    "    \n",
    "    # PASO 1: Regex (s√≠ncono, muy r√°pido)\n",
    "    respuesta_regex = check_regex_response(mensaje)\n",
    "    if respuesta_regex:\n",
    "        return respuesta_regex\n",
    "    \n",
    "    # PASO 2 y 3: RAG y Function Matcher en paralelo\n",
    "    rag_task = asyncio.create_task(buscar_rag_async(mensaje))\n",
    "    function_task = asyncio.create_task(buscar_funcion_async(mensaje))\n",
    "    \n",
    "    rag_result, function_result = await asyncio.gather(rag_task, function_task)\n",
    "    \n",
    "    # Priorizar RAG si encontr√≥ resultado\n",
    "    if rag_result:\n",
    "        return rag_result\n",
    "    \n",
    "    # Usar funci√≥n si se identific√≥\n",
    "    if function_result:\n",
    "        datos = await ejecutar_funcion_async(function_result)\n",
    "        return await naturalizar_async(datos)\n",
    "    \n",
    "    # PASO 4: LLM general\n",
    "    return await llamar_llm_async(mensaje)\n",
    "\n",
    "# Ejecutar\n",
    "# respuesta = await process_user_message_async(\"mensaje del usuario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0d466",
   "metadata": {},
   "source": [
    "## üí° Ventajas del Sistema LLM\n",
    "\n",
    "### ‚úÖ Naturalidad\n",
    "Las respuestas son humanas y contextuales, no rob√≥ticas.\n",
    "\n",
    "### ‚úÖ Flexibilidad\n",
    "Maneja conversaciones fuera del script sin problemas.\n",
    "\n",
    "### ‚úÖ Multiling√ºe\n",
    "Gemini soporta m√∫ltiples idiomas naturalmente.\n",
    "\n",
    "### ‚úÖ Contextual\n",
    "Puede adaptar el tono y contenido seg√∫n la situaci√≥n.\n",
    "\n",
    "### ‚úÖ Actualizable\n",
    "Cambios en prompts se reflejan inmediatamente sin reentrenar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7e9f7",
   "metadata": {},
   "source": [
    "## üìö Recursos y Referencias\n",
    "\n",
    "### APIs y Documentaci√≥n\n",
    "- **Google Gemini**: https://ai.google.dev/\n",
    "- **Gemini API Docs**: https://ai.google.dev/gemini-api/docs\n",
    "- **Python SDK**: https://pypi.org/project/google-generativeai/\n",
    "\n",
    "### Alternativas\n",
    "- **OpenAI GPT-4**: https://platform.openai.com/\n",
    "- **Anthropic Claude**: https://www.anthropic.com/\n",
    "- **Ollama (local)**: https://ollama.ai/\n",
    "\n",
    "### Mejores Pr√°cticas\n",
    "- **Prompt Engineering Guide**: https://www.promptingguide.ai/\n",
    "- **LangChain**: https://python.langchain.com/\n",
    "\n",
    "---\n",
    "\n",
    "*Documentaci√≥n generada para ProyectoAprendizaje - Diciembre 2024*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
