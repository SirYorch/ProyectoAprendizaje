{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81cdc906",
   "metadata": {},
   "source": [
    "##  Arquitectura del Sistema\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────┐\n",
    "│               PIPELINE DE PROCESAMIENTO                  │\n",
    "├──────────────────────────────────────────────────────────┤\n",
    "│                                                          │\n",
    "│  Mensaje Usuario                                         │\n",
    "│       ↓                                                  │\n",
    "│  ┌─────────────────────┐                                │\n",
    "│  │  1. REGEX           │ ← Saludos, despedidas         │\n",
    "│  │  (agent.py)         │   agradecimientos             │\n",
    "│  └─────────────────────┘                                │\n",
    "│       ↓ (si no match)                                    │\n",
    "│  ┌─────────────────────┐                                │\n",
    "│  │  2. RAG             │ ← FAQs de la empresa          │\n",
    "│  │  (embeddings.py)    │                                │\n",
    "│  └─────────────────────┘                                │\n",
    "│       ↓ (si no match)                                    │\n",
    "│  ┌─────────────────────┐                                │\n",
    "│  │  3. FUNCTION        │ ← Predicciones, reportes      │\n",
    "│  │  MATCHER            │                                │\n",
    "│  └─────────────────────┘                                │\n",
    "│       ↓                                                  │\n",
    "│  ┌─────────────────────┐                                │\n",
    "│  │  4. LLM GEMINI      │ ← Naturalización de           │\n",
    "│  │  (llm.py)           │   respuestas                   │\n",
    "│  └─────────────────────┘                                │\n",
    "│       ↓                                                  │\n",
    "│  Respuesta Final                                         │\n",
    "│                                                          │\n",
    "└──────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e80f6c",
   "metadata": {},
   "source": [
    "##  Parte 1: Integración con Google Gemini\n",
    "\n",
    "El archivo `llm.py` contiene la integración con la API de Google Gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2838f3",
   "metadata": {},
   "source": [
    "### Configuración Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ccaf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import os\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Inicializar cliente de Gemini\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7cd8dc",
   "metadata": {},
   "source": [
    "### Función Principal: naturalize_response()\n",
    "\n",
    "Esta función toma datos técnicos y los convierte en respuestas naturales y amigables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0607678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naturalize_response(base, presentation=False):\n",
    "    \"\"\"\n",
    "    Naturaliza respuestas usando Google Gemini.\n",
    "    \n",
    "    Args:\n",
    "        base: Datos técnicos (dict, list, string) a naturalizar\n",
    "        presentation: Si True, genera una presentación del asistente\n",
    "    \n",
    "    Returns:\n",
    "        str: Respuesta naturalizada en lenguaje humano\n",
    "    \n",
    "    Ejemplos:\n",
    "        Base técnica:\n",
    "        {\n",
    "            'producto': 'Laptop HP',\n",
    "            'stock_actual': 45,\n",
    "            'dias_hasta_agotarse': 12\n",
    "        }\n",
    "        \n",
    "        Respuesta naturalizada:\n",
    "        \"Actualmente tenemos 45 unidades de Laptop HP en stock.\n",
    "         Según las ventas recientes, se estima que el producto\n",
    "         se agotará en aproximadamente 12 días.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    if presentation:\n",
    "        # Mensaje de presentación del asistente\n",
    "        message = (\n",
    "            \"Eres un asistente de ventas para una empresa de electrónicos \"\n",
    "            \"llamada Arc -- tienda de electrónicos avanzada, que puede buscar \"\n",
    "            \"en documentos de preguntas frecuentes, o predecir stock, o generar \"\n",
    "            \"reportes, presentate\"\n",
    "        )\n",
    "    else:\n",
    "        # Naturalización de datos\n",
    "        message = (\n",
    "            \"eres un asistente de ventas para una empresa de electrónicos \"\n",
    "            \"de una forma amigable y cómoda, no uses decoradores de texto, \"\n",
    "            \"es decir escribe principalmente lo necesario de forma amigable, \"\n",
    "            \"y si el contenido no existe, da un mensaje de error. \"\n",
    "            f\"Debes presentarle al usuario la siguiente información: {str(base)} \"\n",
    "            \"aqui terminan los datos, si estos se encuentran vacíos, presenta \"\n",
    "            \"un mensaje de error, en lugar de datos incorrectos\"\n",
    "        )\n",
    "    \n",
    "    # Llamada a Gemini\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=message\n",
    "    )\n",
    "    \n",
    "    print(response.text)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e681e92",
   "metadata": {},
   "source": [
    "### Ejemplos de Uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e78448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 1: Presentación del asistente\n",
    "presentacion = naturalize_response(None, presentation=True)\n",
    "print(\"=\"*70)\n",
    "print(presentacion)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ejemplo 2: Naturalizar predicción de stock\n",
    "datos_stock = {\n",
    "    'producto': 'Laptop HP',\n",
    "    'stock_actual': 45,\n",
    "    'prediccion': [\n",
    "        {'fecha': '2024-12-08', 'stock': 42},\n",
    "        {'fecha': '2024-12-09', 'stock': 39},\n",
    "        {'fecha': '2024-12-10', 'stock': 36}\n",
    "    ]\n",
    "}\n",
    "\n",
    "respuesta = naturalize_response(datos_stock)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(respuesta)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ejemplo 3: Naturalizar top productos\n",
    "datos_top = {\n",
    "    'tipo': 'top_vendidos',\n",
    "    'productos': [\n",
    "        {'nombre': 'Mouse Logitech', 'ventas': 156},\n",
    "        {'nombre': 'Teclado Mecánico', 'ventas': 142},\n",
    "        {'nombre': 'Monitor Samsung', 'ventas': 98}\n",
    "    ]\n",
    "}\n",
    "\n",
    "respuesta = naturalize_response(datos_top)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(respuesta)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ejemplo 4: Datos vacíos (manejo de errores)\n",
    "respuesta = naturalize_response({})\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(respuesta)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a690d608",
   "metadata": {},
   "source": [
    "##  Parte 2: Agente Conversacional\n",
    "\n",
    "El archivo `agent.py` combina regex con el procesamiento de mensajes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c8322",
   "metadata": {},
   "source": [
    "### Sistema de Detección por Regex\n",
    "\n",
    "Ya cubierto en el notebook de Expresiones Regulares, pero aquí vemos cómo se integra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf22eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def check_regex_response(user_text: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Primera línea de defensa: respuestas rápidas con regex.\n",
    "    \n",
    "    Retorna:\n",
    "    - str: Respuesta inmediata\n",
    "    - None: Continuar con RAG/Function Matcher/LLM\n",
    "    \"\"\"\n",
    "    text = user_text.lower()\n",
    "\n",
    "    # SALUDOS\n",
    "    patron_saludos = r\"\\b(hola|oli|buenos d[íi]as|buenas tardes|buenas noches|que tal|hello)\\b\"\n",
    "    if re.search(patron_saludos, text):\n",
    "        respuestas = [\n",
    "            \"¡Hola! Bienvenido a Nombre. ¿En qué puedo ayudarte hoy?\",\n",
    "            \"¡Buenas! Soy tu asistente virtual. ¿Buscas stock o información?\",\n",
    "            \"¡Hola! Estoy listo para ayudarte con el inventario.\"\n",
    "        ]\n",
    "        return random.choice(respuestas)\n",
    "\n",
    "    # DESPEDIDAS\n",
    "    patron_despedidas = r\"\\b(chao|chau|adi[óo]s|hasta luego|nos vemos|bye|cu[íi]date)\\b\"\n",
    "    if re.search(patron_despedidas, text):\n",
    "        respuestas = [\n",
    "            \"¡Hasta luego! Gracias por visitar Nombre.\",\n",
    "            \"¡Chao! Vuelve pronto.\",\n",
    "            \"Nos vemos. Espero haberte ayudado.\"\n",
    "        ]\n",
    "        return random.choice(respuestas)\n",
    "\n",
    "    # AGRADECIMIENTOS\n",
    "    patron_agradecimientos = r\"\\b(gracias|te agradezco|muy amable|thx)\\b\"\n",
    "    if re.search(patron_agradecimientos, text):\n",
    "        respuestas = [\n",
    "            \"¡De nada! Es un placer ayudarte.\",\n",
    "            \"¡Para eso estamos!\",\n",
    "            \"Con gusto. ¿Necesitas algo más?\"\n",
    "        ]\n",
    "        return random.choice(respuestas)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5d8c33",
   "metadata": {},
   "source": [
    "##  Pipeline Completo de Procesamiento\n",
    "\n",
    "Así es como se integra todo en el flujo real del chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd2ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_message(mensaje: str) -> str:\n",
    "    \"\"\"\n",
    "    Pipeline completo de procesamiento de mensajes.\n",
    "    \n",
    "    Orden de prioridad:\n",
    "    1. Regex (saludos, despedidas, agradecimientos)\n",
    "    2. RAG (búsqueda en FAQs)\n",
    "    3. Function Matcher (identificar funciones)\n",
    "    4. LLM Gemini (naturalizar respuesta o conversación general)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========================================\n",
    "    # PASO 1: Verificar patrones regex simples\n",
    "    # ========================================\n",
    "    respuesta_regex = check_regex_response(mensaje)\n",
    "    if respuesta_regex:\n",
    "        print(\"[REGEX] Respuesta rápida\")\n",
    "        return respuesta_regex\n",
    "    \n",
    "    # ========================================\n",
    "    # PASO 2: Buscar en RAG (FAQs)\n",
    "    # ========================================\n",
    "    from rag.embeddings import similarity_search\n",
    "    from langchain_ollama import OllamaEmbeddings\n",
    "    \n",
    "    embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    query_vector = embedder.embed_query(mensaje)\n",
    "    \n",
    "    resultados_rag = similarity_search(\n",
    "        query_embedding=query_vector,\n",
    "        top_k=1,\n",
    "        group_filter=\"faq_empresa\",\n",
    "        threshold=0.5\n",
    "    )\n",
    "    \n",
    "    if resultados_rag:\n",
    "        mejor_match, distancia = resultados_rag[0]\n",
    "        print(f\"[RAG] Match encontrado (distancia: {distancia:.4f})\")\n",
    "        return mejor_match.text\n",
    "    \n",
    "    # ========================================\n",
    "    # PASO 3: Function Matcher\n",
    "    # ========================================\n",
    "    from ai.matcher import FunctionCaller\n",
    "    \n",
    "    caller = FunctionCaller()\n",
    "    resultado = caller.identificar_funcion(mensaje)\n",
    "    \n",
    "    if resultado['funcion'] and resultado['confianza'] >= 0.6:\n",
    "        print(f\"[FUNCTION] {resultado['funcion']} identificada\")\n",
    "        \n",
    "        # Ejecutar la función (pseudocódigo)\n",
    "        # datos = ejecutar_funcion(resultado['funcion'], resultado['parametros'])\n",
    "        \n",
    "        # Simular datos de respuesta\n",
    "        datos = {\n",
    "            'funcion': resultado['funcion'],\n",
    "            'parametros': resultado['parametros'],\n",
    "            'resultado': 'Datos de la función...'\n",
    "        }\n",
    "        \n",
    "        # Naturalizar con LLM\n",
    "        print(\"[LLM] Naturalizando respuesta...\")\n",
    "        return naturalize_response(datos)\n",
    "    \n",
    "    # ========================================\n",
    "    # PASO 4: Conversación general con LLM\n",
    "    # ========================================\n",
    "    print(\"[LLM] Generando respuesta conversacional...\")\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=(\n",
    "            \"Eres un asistente de ventas para Arc - tienda de electrónicos. \"\n",
    "            \"Responde de forma amigable y profesional. \"\n",
    "            f\"Usuario: {mensaje}\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3156cec8",
   "metadata": {},
   "source": [
    "##  Pruebas del Sistema Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa130e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casos de prueba diversos\n",
    "test_cases = [\n",
    "    # Regex - Saludos\n",
    "    \"Hola, buenos días\",\n",
    "    \n",
    "    # RAG - FAQs\n",
    "    \"¿Cuál es el horario de atención?\",\n",
    "    \"¿Aceptan devoluciones?\",\n",
    "    \n",
    "    # Function Matcher\n",
    "    \"¿Cuánto stock hay de Laptop HP?\",\n",
    "    \"Productos más vendidos\",\n",
    "    \"Stock para el 25 de diciembre\",\n",
    "    \n",
    "    # LLM General\n",
    "    \"¿Por qué debería comprar en su tienda?\",\n",
    "    \"Cuéntame sobre laptops\",\n",
    "    \n",
    "    # Regex - Despedidas\n",
    "    \"Gracias, adiós\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PRUEBA COMPLETA DEL SISTEMA LLM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, mensaje in enumerate(test_cases, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{i}/{len(test_cases)}]  Usuario: '{mensaje}'\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    respuesta = process_user_message(mensaje)\n",
    "    \n",
    "    print(f\"\\n Asistente:\\n{respuesta}\")\n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a065020",
   "metadata": {},
   "source": [
    "##  Métricas de Rendimiento\n",
    "\n",
    "### Comparación de Tiempos de Respuesta\n",
    "\n",
    "| Método | Tiempo Promedio | Costo | Precisión |\n",
    "|--------|----------------|-------|------------|\n",
    "| **Regex** | < 1ms | $0.00 | 100% (patrones exactos) |\n",
    "| **RAG** | 10-50ms | $0.00 | 85-95% |\n",
    "| **Function Matcher** | 20-80ms | $0.00 | 85-92% |\n",
    "| **LLM Gemini** | 500-2000ms | $0.001-0.01 | 90-98% |\n",
    "\n",
    "### Distribución de Uso (Ejemplo)\n",
    "\n",
    "```\n",
    "Regex:           15% ←─ Respuestas instantáneas\n",
    "RAG:             35% ←─ FAQs y conocimiento base\n",
    "Function Match:  40% ←─ Funcionalidad principal\n",
    "LLM:            10% ←─ Conversación general\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3368bd76",
   "metadata": {},
   "source": [
    "##  Configuración y Variables de Entorno\n",
    "\n",
    "### Archivo .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60305621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env\n",
    "\"\"\"\n",
    "# Google Gemini API\n",
    "GOOGLE_GEMINI_API_KEY=tu_api_key_aqui\n",
    "\n",
    "# PostgreSQL\n",
    "DATABASE_URL=postgresql://usuario1:password1@localhost:5432/aprendizaje\n",
    "\n",
    "# Ollama (para embeddings)\n",
    "OLLAMA_BASE_URL=http://localhost:11434\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5a92a9",
   "metadata": {},
   "source": [
    "### Instalación de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a977a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txt\n",
    "\"\"\"\n",
    "google-generativeai\n",
    "python-dotenv\n",
    "langchain-ollama\n",
    "sentence-transformers\n",
    "sqlalchemy\n",
    "pgvector\n",
    "psycopg2-binary\n",
    "\"\"\"\n",
    "\n",
    "# Instalación\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b1626",
   "metadata": {},
   "source": [
    "##  Personalización del Asistente\n",
    "\n",
    "### Ajustar Personalidad y Tono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5575668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sistema de prompts modulares\n",
    "\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"base\": (\n",
    "        \"Eres un asistente de ventas para Arc - tienda de electrónicos avanzada. \"\n",
    "    ),\n",
    "    \n",
    "    \"tono_amigable\": (\n",
    "        \"Usa un tono amigable, cercano y profesional. \"\n",
    "        \"Sé conciso pero completo en tus respuestas. \"\n",
    "    ),\n",
    "    \n",
    "    \"formato\": (\n",
    "        \"No uses decoradores de texto como **negrita** o *cursiva*. \"\n",
    "        \"Escribe en texto plano pero bien estructurado. \"\n",
    "    ),\n",
    "    \n",
    "    \"manejo_errores\": (\n",
    "        \"Si no tienes información o los datos están vacíos, \"\n",
    "        \"comunica el error de forma amable y ofrece alternativas. \"\n",
    "    )\n",
    "}\n",
    "\n",
    "def build_prompt(datos, tipo=\"respuesta\"):\n",
    "    \"\"\"Construye prompt personalizado según el tipo de interacción\"\"\"\n",
    "    \n",
    "    base = SYSTEM_PROMPTS[\"base\"]\n",
    "    tono = SYSTEM_PROMPTS[\"tono_amigable\"]\n",
    "    formato = SYSTEM_PROMPTS[\"formato\"]\n",
    "    errores = SYSTEM_PROMPTS[\"manejo_errores\"]\n",
    "    \n",
    "    if tipo == \"presentacion\":\n",
    "        return f\"{base} {tono} Preséntate brevemente al usuario.\"\n",
    "    \n",
    "    elif tipo == \"respuesta\":\n",
    "        return (\n",
    "            f\"{base} {tono} {formato} {errores} \"\n",
    "            f\"Presenta la siguiente información al usuario: {datos}\"\n",
    "        )\n",
    "    \n",
    "    elif tipo == \"conversacion\":\n",
    "        return f\"{base} {tono} Responde a la consulta del usuario.\"\n",
    "\n",
    "# Uso\n",
    "# prompt = build_prompt(datos_stock, tipo=\"respuesta\")\n",
    "# response = client.models.generate_content(model=\"gemini-2.5-flash\", contents=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716f4d9",
   "metadata": {},
   "source": [
    "##  Mejores Prácticas de Seguridad\n",
    "\n",
    "### 1. Manejo Seguro de API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80329e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# NUNCA hardcodear API keys\n",
    "#  MAL\n",
    "# api_key = \"AIzaSyC123456789...\"\n",
    "\n",
    "#  BIEN\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_GEMINI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key no configurada en .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd573e3",
   "metadata": {},
   "source": [
    "### 2. Validación de Entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc1315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_user_input(mensaje: str) -> bool:\n",
    "    \"\"\"\n",
    "    Valida entrada del usuario antes de procesarla.\n",
    "    \"\"\"\n",
    "    # Longitud máxima\n",
    "    if len(mensaje) > 500:\n",
    "        raise ValueError(\"Mensaje demasiado largo (máx. 500 caracteres)\")\n",
    "    \n",
    "    # No vacío\n",
    "    if not mensaje.strip():\n",
    "        raise ValueError(\"Mensaje vacío\")\n",
    "    \n",
    "    # Detectar posibles inyecciones (básico)\n",
    "    palabras_prohibidas = [\"<script>\", \"DROP TABLE\", \"DELETE FROM\"]\n",
    "    mensaje_lower = mensaje.lower()\n",
    "    \n",
    "    for palabra in palabras_prohibidas:\n",
    "        if palabra.lower() in mensaje_lower:\n",
    "            raise ValueError(\"Entrada inválida detectada\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568ef05",
   "metadata": {},
   "source": [
    "### 3. Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"\n",
    "    Limita la cantidad de peticiones por usuario.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_requests=10, time_window=60):\n",
    "        self.max_requests = max_requests  # Máximo de peticiones\n",
    "        self.time_window = time_window    # Ventana de tiempo (segundos)\n",
    "        self.requests = defaultdict(list)\n",
    "    \n",
    "    def is_allowed(self, user_id: str) -> bool:\n",
    "        \"\"\"Verifica si el usuario puede hacer otra petición\"\"\"\n",
    "        now = datetime.now()\n",
    "        \n",
    "        # Limpiar peticiones antiguas\n",
    "        cutoff = now - timedelta(seconds=self.time_window)\n",
    "        self.requests[user_id] = [\n",
    "            req_time for req_time in self.requests[user_id]\n",
    "            if req_time > cutoff\n",
    "        ]\n",
    "        \n",
    "        # Verificar límite\n",
    "        if len(self.requests[user_id]) >= self.max_requests:\n",
    "            return False\n",
    "        \n",
    "        # Registrar nueva petición\n",
    "        self.requests[user_id].append(now)\n",
    "        return True\n",
    "\n",
    "# Uso\n",
    "limiter = RateLimiter(max_requests=10, time_window=60)\n",
    "\n",
    "# if not limiter.is_allowed(user_id):\n",
    "#     return \"Has excedido el límite de peticiones. Intenta en un minuto.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0b728",
   "metadata": {},
   "source": [
    "## Monitoreo y Logging\n",
    "\n",
    "### Sistema de Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35446120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('chatbot.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_interaction(user_id: str, mensaje: str, respuesta: str, metodo: str):\n",
    "    \"\"\"\n",
    "    Registra cada interacción del chatbot.\n",
    "    \"\"\"\n",
    "    logger.info(f\"\"\"\\n\n",
    "    {'='*50}\n",
    "    INTERACCIÓN\n",
    "    {'='*50}\n",
    "    Timestamp: {datetime.now().isoformat()}\n",
    "    User ID: {user_id}\n",
    "    Método: {metodo}\n",
    "    \n",
    "    Mensaje: {mensaje}\n",
    "    Respuesta: {respuesta[:100]}...\n",
    "    {'='*50}\n",
    "    \"\"\")\n",
    "\n",
    "# Uso en el pipeline\n",
    "# log_interaction(user_id, mensaje, respuesta, \"REGEX\")\n",
    "# log_interaction(user_id, mensaje, respuesta, \"RAG\")\n",
    "# log_interaction(user_id, mensaje, respuesta, \"FUNCTION_MATCHER\")\n",
    "# log_interaction(user_id, mensaje, respuesta, \"LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e120517",
   "metadata": {},
   "source": [
    "##  Optimizaciones de Rendimiento\n",
    "\n",
    "### 1. Caché de Respuestas Frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import hashlib\n",
    "\n",
    "class ResponseCache:\n",
    "    \"\"\"\n",
    "    Caché simple para respuestas frecuentes.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size=100):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def _hash_message(self, mensaje: str) -> str:\n",
    "        \"\"\"Genera hash del mensaje para usar como key\"\"\"\n",
    "        return hashlib.md5(mensaje.lower().strip().encode()).hexdigest()\n",
    "    \n",
    "    def get(self, mensaje: str):\n",
    "        \"\"\"Obtiene respuesta del caché\"\"\"\n",
    "        key = self._hash_message(mensaje)\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, mensaje: str, respuesta: str):\n",
    "        \"\"\"Guarda respuesta en caché\"\"\"\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            # Eliminar el más antiguo (FIFO simple)\n",
    "            self.cache.pop(next(iter(self.cache)))\n",
    "        \n",
    "        key = self._hash_message(mensaje)\n",
    "        self.cache[key] = respuesta\n",
    "\n",
    "# Uso\n",
    "cache = ResponseCache(max_size=100)\n",
    "\n",
    "def process_with_cache(mensaje: str) -> str:\n",
    "    # Verificar caché\n",
    "    cached = cache.get(mensaje)\n",
    "    if cached:\n",
    "        logger.info(\"[CACHE] Respuesta encontrada en caché\")\n",
    "        return cached\n",
    "    \n",
    "    # Procesar normalmente\n",
    "    respuesta = process_user_message(mensaje)\n",
    "    \n",
    "    # Guardar en caché\n",
    "    cache.set(mensaje, respuesta)\n",
    "    \n",
    "    return respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a444a0f",
   "metadata": {},
   "source": [
    "### 2. Procesamiento Asíncrono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc15b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def process_user_message_async(mensaje: str) -> str:\n",
    "    \"\"\"\n",
    "    Versión asíncrona del pipeline.\n",
    "    Permite procesar múltiples mensajes concurrentemente.\n",
    "    \"\"\"\n",
    "    \n",
    "    # PASO 1: Regex (síncono, muy rápido)\n",
    "    respuesta_regex = check_regex_response(mensaje)\n",
    "    if respuesta_regex:\n",
    "        return respuesta_regex\n",
    "    \n",
    "    # PASO 2 y 3: RAG y Function Matcher en paralelo\n",
    "    rag_task = asyncio.create_task(buscar_rag_async(mensaje))\n",
    "    function_task = asyncio.create_task(buscar_funcion_async(mensaje))\n",
    "    \n",
    "    rag_result, function_result = await asyncio.gather(rag_task, function_task)\n",
    "    \n",
    "    # Priorizar RAG si encontró resultado\n",
    "    if rag_result:\n",
    "        return rag_result\n",
    "    \n",
    "    # Usar función si se identificó\n",
    "    if function_result:\n",
    "        datos = await ejecutar_funcion_async(function_result)\n",
    "        return await naturalizar_async(datos)\n",
    "    \n",
    "    # PASO 4: LLM general\n",
    "    return await llamar_llm_async(mensaje)\n",
    "\n",
    "# Ejecutar\n",
    "# respuesta = await process_user_message_async(\"mensaje del usuario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0d466",
   "metadata": {},
   "source": [
    "##  Ventajas del Sistema LLM\n",
    "\n",
    "###  Naturalidad\n",
    "Las respuestas son humanas y contextuales, no robóticas.\n",
    "\n",
    "###  Flexibilidad\n",
    "Maneja conversaciones fuera del script sin problemas.\n",
    "\n",
    "###  Multilingüe\n",
    "Gemini soporta múltiples idiomas naturalmente.\n",
    "\n",
    "###  Contextual\n",
    "Puede adaptar el tono y contenido según la situación.\n",
    "\n",
    "###  Actualizable\n",
    "Cambios en prompts se reflejan inmediatamente sin reentrenar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7e9f7",
   "metadata": {},
   "source": [
    "##  Recursos y Referencias\n",
    "\n",
    "### APIs y Documentación\n",
    "- **Google Gemini**: https://ai.google.dev/\n",
    "- **Gemini API Docs**: https://ai.google.dev/gemini-api/docs\n",
    "- **Python SDK**: https://pypi.org/project/google-generativeai/\n",
    "\n",
    "### Alternativas\n",
    "- **OpenAI GPT-4**: https://platform.openai.com/\n",
    "- **Anthropic Claude**: https://www.anthropic.com/\n",
    "- **Ollama (local)**: https://ollama.ai/\n",
    "\n",
    "### Mejores Prácticas\n",
    "- **Prompt Engineering Guide**: https://www.promptingguide.ai/\n",
    "- **LangChain**: https://python.langchain.com/\n",
    "\n",
    "---\n",
    "\n",
    "*Documentación generada para ProyectoAprendizaje - Diciembre 2024*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
